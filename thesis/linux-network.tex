\chapter{Overview of the Linux Network Stack}

\begin{wrapfigure}[20]{r}{.35\textwidth}
	\centering
	\includegraphics[scale=1]{images/net-stack.pdf}
	\caption{Linux network stack.}
	\label{fig:net-stack}
\end{wrapfigure}
The top most layer of the Linux network stack is the \textbf{system call interface} to create and operate on sockets \cite{netstackana}. The usage of this interface by \textit{nginx} was already described in section \ref{sec:nginx-os-if}. Its purpose is to multiplex networking calls by the user into the kernel \cite{netstackana}. Once a file descriptor was created using the socket interface, data can be exchanged with the network system through general operations on file descriptors like \textit{write} and \textit{read}, too. The socket interface is completely agnostic to different protocols or implementations by network devices and drivers. Its main underlying structure is "\texttt{struct sock}", containing all relevant information about a specific socket, like available functions and protocol specific state information.\footnote{\url{http://www.ecsl.cs.sunysb.edu/elibrary/linux/network/LinuxKernel.pdf}} Thereby also protocol and implementation specific functionality is bind to a socket using function pointers (as defined in "\texttt{struct inet\_protosw}") \cite{netstackana}.

Whereas the more general data structure "\texttt{struct sock}" contains mainly meta information about a socket, the major structure for storing data is "\texttt{struct sk\_buff}". \textbf{\texttt{sk\_buff}} contains packet data and state information. It is used for to be sent, as well as for received packets almost through out all layers of the network stack. \cite{netstackana}

The gap between protocol handling and device drivers is bridged by the \textbf{network interface} (\textit{netif}) layer. This is a hardware device "agnostic interface layer" \cite{netstackana}. Its major purpose is to connect protocols to hardware devices. Information about devices is provided through "\texttt{struct net\_device}". On system start-up all available devices register themselves with a filled out "\texttt{net\_device}" structure. From there on they are know to the network interface layer.

The lowest layer of the network stack being part of the Linux kernel is build by \textbf{device drivers}. These hook into the network interface layer and manage physical/hardware network controllers. For the implemented System on Chip this is the \texttt{xps\_ll\_temac} \gls{ip} core.

\begin{wrapfigure}[10]{r}{.23\textwidth}
	\centering
	\includegraphics[scale=1]{images/netif-dev.pdf}
	\caption{\texttt{sk\_buff} exchange}
	\label{fig:netif-dev}
\end{wrapfigure}

Since introduction of the "New API" (\textit{NAPI}) for communication between device drivers processing packets and the network interface layer, most of the workload for received and to be sent packets is scheduled using software interrupt requests (also known as "\textit{soft irq}" or "\textit{sirq}").\footnote{\url{http://www.linuxfoundation.org/collaborate/workgroups/networking/napi} (as of 12/2012)} During performance tests described in the previous chapter this was visible through high CPU utilization of the \textit{sirq} category.

The network interface layer enqueues \texttt{sk\_buff} packets for transmission using the function \texttt{int hard\_start\_xmit(struct sk\_buff *skb, struct net\_device *dev)}. Usually a call to this function is protected with a \textit{lock} to protect it from being called multiple times simultaneously.\footnote{\url{http://lwn.net/Articles/120960/} (as of 12/2012)} The device driver pushes received packets to the upper layer using \texttt{int netif\_receive\_skb(struct sk\_buff *skb)}. \cite{netstackana}

\chapter{TCP Offload Engine}

\section{Overview}

\subsection{Benefits}

Conducted performance tests showed\footnote{see figures \ref{fig:initial-req-cpu} and \ref{fig:tcp-io-perf-cpu}} that much processing time is spend handling interrupt requests of the network sub system. Especially on TCP-heavy-requests this circumstance tuned out to be a bottleneck for a system with little processing power like the designed System on Chip.

On the other hand is handling of TCP connections and processing messages a well defined and general problem that could be outsourced to an own "processor" leaving more processing power of the main system for other tasks. This is the approach of \textit{TCP Offload Engines} (TOE).
\\

\subsection{State of the Art}

Throughout recent years some attempts by companies manufacturing network interface cards were made to integrate support for \textit{TCP Offload Engines} into the Linux kernel (e.g. by \textit{Chelsio}: \url{http://lwn.net/Articles/147289/}). However, maintainers\footnote{\url{http://lwn.net/Articles/148701/}} of the Linux network stack and the \textit{Linux Foundation}\footnote{\url{http://www.linuxfoundation.org/collaborate/workgroups/networking/toe}} have a strong opinion against integrating drivers and support through out the network stack for these hardware devices.

This is partially based on reservations about quality standards and the lack of possible benefits, but includes also arguments of a more general nature. A \textit{TOE} would "short out much of the Linux networking code" and therefore "cut out little features like \textit{netfilter}, traffic control, and more" \cite{linux-toe}. Additionally it is not easily possible to supply security fixes for \textit{TOE} functionality implemented in hardware. Therefore it would be necessary to cut off integration of a \textit{TOE} on occurring security vulnerabilities by releasing Linux kernel hot fix versions.

These are reasons why no support for \textit{TCP Offload Engines} is available in Linux kernel, currently. Of course all arguments are based on the point of view of Linux being a general purpose operating system used in desktop and server systems, powered by decent computer hardware. For embedded systems these assumptions are usually not fulfilled. Neither will the \textit{TOE} be part of an exchangeable and replaceable network interface card, supplied by a third-party vendor, but probably integrated into a System on Chip as an \gls{ip} core. Therefore the next sections will discuss approaches for an integration of dedicated engines, offloading TCP processing to hardware.
\\

\section{Possible Approaches}

There are a number of levels for offloading features to hardware. The simplest one is offloading checksum calculation to the network interface card. This is already implemented by the used \textit{xps\_ll\_temac} \gls{ip} core (see sec. \ref{preceeding:net}).
\\

\subsection{Large Segment and Receive Offload (LSO/LRO)}

Another option which is already supported by the network device layer of the Linux kernel is \textit{TCP Segmentation Offload} (TSO), also known as \textit{Large Segment Offload} (LSO).\footnote{\url{http://www.linuxfoundation.org/collaborate/workgroups/networking/tso} (as of 12/2012)} Inside of the Linux kernel, this technique uses very large data buffers in a single \texttt{sk\_buff} structure and therefore large \gls{tcp} packets far beyond typical \gls{mtu} values. Splitting these single packets to multiple packets of transmittable size (i.e. segmentation) is done by the network device, connected to the Linux kernel as a hardware device. This reduces the number of processed TCP packets and acknowledgments by the CPU, but leaves all state related tasks to the operating system. Therefore it prevents short cutting the network stack and additional functionality for network management and analysis. But it implicates known problems of transparent segmentation, too \cite{kn1}[sec. "5.5.7 Segmentation"].

The other way round is called \textit{Large Receive Offload} (LRO). This concept is supported by the \textit{New API} (NAPI) in Linux kernel for receiving network packets in the way that the number of hardware interrupt requests is reduced on high network utilization, but implemented completely in software \cite{linux-lro}.
\\

\subsubsection{TCP Chimney Offload}

Microsoft has introduced a technique called \textit{TCP Chimney Offload} with \textit{Microsoft\textsuperscript{\textregistered} Windows Server\textsuperscript{\textregistered} 2003 Scalable Networking Pack (SNP)}. This allows offloading the complete handling of TCP processing "on demand" to the network interface card. Connections are therefore known to the operating system and can be managed complete in software. But it is also possible to offload further processing of a connection to hardware to alleviate bottlenecks. Because not only packet, but also connection and state handling can be done by the hardware, the network interface card requires a complete implementation of the TCP/IP protocol stack. \cite{dell-toe}

This is a proprietary solution by Microsoft and some hardware vendors. There are no approaches of bringing it to the Linux kernel or other unix-based operating systems, to the knowledge of this report's author.
\\

\subsubsection{Full-stack TCP Offload}

The last to be introduced solution can be described as "\textit{Full-stack TCP Offload}". All processing of \gls{tcp} and \gls{ip} concerns is done by hardware. This requires a great deal of protocol knowledge and functionality to be implemented in hardware, but promises also the best performance gains, leaving just a minimal part of TCP/IP stack processing to the operating system.

The next section contains some considerations for integrating such a \textit{TOE} into the Linux kernel and remarks concerning device driver development and interfaces of the \textit{TOE}.
\\

\section{Integrating a Custom \textit{Full-stack} TOE}

\subsubsection{Direct Memory Access (DMA)}